import os
import re
import json
import jieba
from dotenv import load_dotenv

if not os.getenv('GITHUB_ACTIONS'):
    load_dotenv(dotenv_path='.sora.env')
from peewee import *
from model.mysql_models import (
    DB_MYSQL, Video, Document, SoraContent, Sora, SoraMedia, FileTag, Tag, init_mysql
)
from database import ensure_connection
from model.scrap import Scrap

SYNC_TO_POSTGRES = os.getenv('SYNC_TO_POSTGRES', 'false').lower() == 'true'
BATCH_LIMIT = None
# åˆå§‹åŒ– MySQLï¼ˆå¿…é¡»å…ˆæ‰§è¡Œï¼‰
init_mysql()

# å¦‚éœ€ PostgreSQLï¼Œå†å¯¼å…¥å¹¶åˆå§‹åŒ–
if SYNC_TO_POSTGRES:
    from model.pg_models import DB_PG, SoraContentPg, SoraMediaPg, init_postgres
    from playhouse.shortcuts import model_to_dict
    init_postgres()
    # try:
    #     DB_PG.connect()
    #     print("âœ”ï¸ DSN æ–¹å¼ä¸‹ï¼Œconnect() æˆåŠŸï¼")
    #     cursor = DB_PG.execute_sql("SELECT 1;")
    #     print("[test query] SELECT 1 è¿”å›ï¼š", cursor.fetchone()[0])
    # except Exception as e:
    #     print("âŒ DSN æ–¹å¼ä¸‹ï¼Œconnect() å¤±è´¥ï¼š", e)
    # finally:
    #     if not DB_PG.is_closed():
    #         DB_PG.close()
    #         print("ğŸ”’ è¿æ¥å·²å…³é—­")

# åŒä¹‰è¯å­—å…¸
SYNONYM = {
    "æ»‘é¼ ": "é¼ æ ‡",
    "è¤å¹•": "æ˜¾ç¤ºå™¨",
    "ç¬”ç”µ": "ç¬”è®°æœ¬",
}

def clean_bj_text(original_string):
    target_strings = ["ğŸ’¾"]
    for target in target_strings:
        pos = original_string.find(target)
        if pos != -1:
            original_string = original_string[:pos]
    return original_string

def clean_text(original_string):
    target_strings = ["- Advertisement - No Guarantee", "- å¹¿å‘Š - æ— æ‹…ä¿"]
    for target in target_strings:
        pos = original_string.find(target)
        if pos != -1:
            original_string = original_string[:pos]

    replace_texts = [
        "æ±‚æ‰“èµ", "æ±‚èµ", "å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æˆ–åˆ†äº«æ–‡ä»¶",
        "ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº @datapanbot å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
        "â‘ ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº  å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
        "å•æœºå¤åˆ¶ï¼š", "æ–‡ä»¶è§£ç å™¨:", "æ‚¨çš„æ–‡ä»¶ç å·²ç”Ÿæˆï¼Œç‚¹å‡»å¤åˆ¶ï¼š",
        "æ‰¹é‡å‘é€çš„åª’ä½“ä»£ç å¦‚ä¸‹:", "æ­¤æ¡åª’ä½“åˆ†äº«link:",
        "å¥³ä¾…æœç´¢ï¼š@ seefilebot", "è§£ç ï¼š@ MediaBK2bot",
        "å¦‚æœæ‚¨åªæ˜¯æƒ³å¤‡ä»½ï¼Œå‘é€ /settings å¯ä»¥è®¾ç½®å…³é—­æ­¤æ¡å›å¤æ¶ˆæ¯",
        "åª’ä½“åŒ…å·²åˆ›å»ºï¼", "æ­¤åª’ä½“ä»£ç ä¸º:", "æ–‡ä»¶åç§°:", "åˆ†äº«é“¾æ¥:", "|_SendToBeach_|",
        "Forbidden: bot was kicked from the supergroup chat",
        "Bad Request: chat_id is empty"
    ]
    for text in replace_texts:
        original_string = original_string.replace(text, '')

    original_string = re.sub(r"åˆ†äº«è‡³\d{4}-\d{2}-\d{2} \d{2}:\d{2} åˆ°æœŸåæ‚¨ä»å¯é‡æ–°åˆ†äº«", '', original_string)

    json_pattern = r'\{[^{}]*?"text"\s*:\s*"[^"]+"[^{}]*?\}'
    matches = re.findall(json_pattern, original_string)
    for match in matches:
        try:
            data = json.loads(match)
            if 'content' in data and isinstance(data['content'], str):
                original_string += f"\n{data['content']}"
        except json.JSONDecodeError:
            pass
        original_string = original_string.replace(match, '')

    wp_patterns = [r'https://t\.me/[^\s]+']
    for pattern in wp_patterns:
        original_string = re.sub(pattern, '', original_string)

    for pat in [
        r'LINK\s*\n[^\n]+#C\d+\s*\nOriginal:[^\n]*\n?',
        r'LINK\s*\n[^\n]+#C\d+\s*\nForwarded from:[^\n]*\n?',
        r'LINK\s*\n[^\n]*#C\d+\s*',
        r'Original caption:[^\n]*\n?'
    ]:
        original_string = re.sub(pat, '', original_string)

    original_string = re.sub(r'^\s*$', '', original_string, flags=re.MULTILINE)
    lines = original_string.split('\n')
    unique_lines = list(dict.fromkeys(lines))
    result_string = "\n".join(lines)

    for symbol in ['ğŸ”‘', 'ğŸ’']:
        result_string = result_string.replace(symbol, '\r\n' + symbol)

    return result_string[:1500] if len(result_string) > 1500 else result_string

def replace_synonym(text):
    for k, v in SYNONYM.items():
        text = text.replace(k, v)
    return text

def segment_text(text):
    text = replace_synonym(text)
    return " ".join(jieba.cut(text))

def fetch_tag_cn_for_file(file_unique_id):
    return [
        t.tag_cn for t in Tag.select()
        .join(FileTag, on=(FileTag.tag == Tag.tag))
        .where(FileTag.file_unique_id == file_unique_id)
        if t.tag_cn
    ]

def sync_to_postgres(record):
    if not SYNC_TO_POSTGRES:
        return

    from playhouse.shortcuts import model_to_dict

    IGNORED_FIELDS = {'content_seg_tsv', 'created_at', 'updated_at'}

    model_data = model_to_dict(record, recurse=False)
    model_data = {k: v for k, v in model_data.items() if k not in IGNORED_FIELDS}
    model_data["id"] = record.id  # æ˜¾å¼ä¸»é”®

    with DB_PG.atomic():
        try:
            existing = SoraContentPg.get(SoraContentPg.id == record.id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()
        except SoraContentPg.DoesNotExist:
            SoraContentPg.create(**model_data)

def sync_media_to_postgres(content_id, media_rows):
    if not SYNC_TO_POSTGRES:
        return

    with DB_PG.atomic():
        for media in media_rows:
            insert_data = {
                "content_id": content_id,
                "source_bot_name": media["source_bot_name"],
                "file_id": media["file_id"],
                "thumb_file_id": media["thumb_file_id"]
            }
            print(f"Syncing media to PostgreSQL: {insert_data}")

            try:
                print(f"ğŸ›°ï¸ Syncing media to PostgreSQL: {insert_data}")

                SoraMediaPg.insert(**insert_data).on_conflict(
                    conflict_target=[SoraMediaPg.content_id, SoraMediaPg.source_bot_name],
                    update={k: insert_data[k] for k in ['file_id', 'thumb_file_id']}
                ).execute()

            except Exception as e:
                print(f"âŒ æ’å…¥ PostgreSQL sora_media å¤±è´¥: {e}")
                print(f"   â¤ å¤±è´¥å†…å®¹: {insert_data}")

def process_documents():
    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•
    if SYNC_TO_POSTGRES:



        DB_PG.connect()

    print("ğŸš€ å¼€å§‹åŒæ­¥ stage != 'updated' çš„ document åˆ° PostgreSQL...",flush=True)
    for doc in Document.select().where((Document.kc_status.is_null(True)) | (Document.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        # æ–‡æœ¬æ¸…æ´—ä¸åˆ†è¯
        content = clean_text(f"{doc.file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)

        # æ ‡ç­¾åˆ†è¯è¿½åŠ 
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}")

        # ç»Ÿä¸€è®°å½•æ•°æ®
        record_data = {
            'source_id': doc.file_unique_id,
            'file_type': 'd',
            'content': content,
            'content_seg': content_seg,
            'file_size': doc.file_size
        }

        # ä½¿ç”¨ get_or_create ä¿è¯å”¯ä¸€æ€§ï¼Œé¿å… Duplicate
        kw, created = SoraContent.get_or_create(source_id=doc.file_unique_id, defaults=record_data)

        if not created:
            # å·²å­˜åœ¨ï¼Œæ›´æ–°å­—æ®µ
            for key, value in record_data.items():
                setattr(kw, key, value)
            kw.save()

        # æ›´æ–° Document è®°å½•
        doc.kc_id = kw.id
        doc.kc_status = 'updated'
        doc.save()

        # åŒæ­¥ PostgreSQL
        if SYNC_TO_POSTGRES and kw.id:
            sync_to_postgres(kw)

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()


def process_videos():
    ensure_connection()  # âœ… æ¨èå†™æ³•
    # DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    print("ğŸš€ å¼€å§‹åŒæ­¥ stage != 'updated' çš„ video åˆ° PostgreSQL...")
    for doc in Video.select().where((Video.kc_status.is_null(True)) | (Video.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        content = clean_text(f"{doc.file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}: {content_seg}")

        record_data = {
            'source_id': doc.file_unique_id,
            'file_type': 'v',
            'content': content,
            'content_seg': content_seg,
            'file_size': doc.file_size,
            'duration': doc.duration
        }

        kw, created = SoraContent.get_or_create(source_id=doc.file_unique_id, defaults=record_data)

        if not created:
            for key, value in record_data.items():
                setattr(kw, key, value)
            kw.save()

        print(f"  ğŸ”„ æ›´æ–° MySQL sora_content [{kw}]")

        print(kw.__data__)

        doc.kc_id = kw.id
        doc.kc_status = 'updated'
        doc.save()

        if SYNC_TO_POSTGRES and kw.id:
            sync_to_postgres(kw)

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()


def parse_bj_tag_for_file(tag_str):
    tag_cn_list = []
    if tag_str:
       # å°† tag_str æŒ‰ç©ºç™½ç¬¦å·åˆ†å‰²
        tag_list = tag_str.split()
        for tag in tag_list:
            # ç§»é™¤ tag å‰çš„ #
            tag = tag.lstrip('#')
            
            tag_mapping = {
                "ç™½ç§äºº": "ç™½äºº",
                "é»‘ç§äºº": "é»‘äºº",
                "éœ²è„¸": "æœ‰éœ²è„¸",
                "é®æŒ¡": "å¸¦äº†é¢ç½©",
                "æœªéœ²è„¸": "æ²¡æœ‰éœ²è„¸",
                "æ— æ¯›": "é«˜å¹´çº§_å°äº”",
                "ä¸­æ¯›": "å°‘å¹´_é«˜ä¸­",
                "é»‘æ£®æ—": "å°‘å¹´_é«˜ä¸­",
                "å¹¼å„¿": "ä½å¹´çº§_å°äºŒ",
                "å°å­¦": "ä½å¹´çº§_å°äºŒ",
                "åˆä¸­": "åˆæ¯›",
                "é«˜ä¸­": "å°‘å¹´_é«˜ä¸­",
                "åˆæ³•": "å°‘å¹´_é«˜ä¸­",
                "æ¸…æ°´": "æ²¡æœ‰è£¸ä½“",
                "å†™çœŸ": "æ­£å¤ªä¸»é¢˜æ±‡æ•´",
                "åŠ¨ç”»": "å¡é€šåŠ¨æ¼«",
                "æ¯å­": "æ­£å¤ªä¸é˜¿å§¨",
                "è‚›äº¤": "çˆ†èŠ",
                "å£äº¤": "å£äº¤",
                "è¶³äº¤": "æ‹è¶³",
                "è‡ªæ’¸": "æ’¸ç®¡",
                "å°„ç²¾": "å°„ç²¾",
                "å±•ç¤º": "æ­£å¤ªç‹¬ç§€",
                "æ‘¸": "æ‰‹äº¤",
                "å·æ‹": "å·æ‹",
                "èƒ–å¤ª": "èƒ–å¤ª",
                "TKæŒ ç—’": "ç˜™ç—’",
                "SPæ‰“å±è‚¡": "æ‰“å±è‚¡",
                "æ‹ç‰©": "æ‹ç‰©",
                "çŒå¥‡é‡å£": "çŒå¥‡",
                "åŒ»å­¦ç±»": "åŒ»å­¦",
                "éœ¸å‡Œ": "éœ¸å‡Œ",
                "ç›‘è§†": "ç›‘è§†å™¨",
                "ç›´æ’­å½•å±": "ç›´æ’­",
                "å†°æ·‡æ·‹": "å†°æ·‡æ·‹",
                "å¥¶é»„åŒ…": "å¥¶é»„åŒ…",
                "çœ¼é•œå“¥": "çœ¼é•œå“¥ç³»åˆ—",
                "è¥¿è¾¹çš„é£":"è¥¿è¾¹çš„é£",
                "æ—¥æœ¬å…¨æ–¹ä½":"æ—¥æœ¬å…¨æ–¹ä½",
                "å°å­©ä¸ç¬¨":"å°å­©ä¸ç¬¨",  #
                "ç½‘è°ƒå¤§ç¥":"ç½‘è°ƒå¤§ç¥",  #
                "çŒ«ç³»åˆ—":"çŒ«ç³»åˆ—",  
                "é›¨èŠ±çŸ³":"é›¨èŠ±çŸ³ç³»åˆ—",  
                "æ—©ç‚¹ç¡è§‰caodidi":"Caodidiç³»åˆ—",
                "BBåµ¬":"BBåµ¬",
                "weå‡ºå“":"WEç³»åˆ—",
                "å ‚å±±å¤©è‰":"å ‚å±±",
                "ä¹ˆä¹ˆå“’è§†é¢‘":"ä¹ˆä¹ˆå“’è§†é¢‘", #
                "ä¹ˆä¹ˆå“’ä¸¾ç‰ŒåŸåˆ›":"ä¹ˆä¹ˆå“’ä¸¾ç‰ŒåŸåˆ›", #
                "å°6ç«¥æ¨¡":"å°å…­æ‘„å½±",
                "lediäºŒç»´ç ":"äºŒç»´ç ç³»åˆ—"
            }

            # æ›¿æ¢æ ‡ç­¾
            tag = tag_mapping.get(tag, tag)  # å¦‚æœ tag ä¸åœ¨æ˜ å°„ä¸­ï¼Œå°±ä¿ç•™åŸå€¼
            tag_cn_list.append(tag)

    return tag_cn_list

def process_scrap():
    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    print("ğŸš€ å¼€å§‹åŒæ­¥ stage != 'updated' çš„ scrap åˆ° PostgreSQL...")
    for scrap in Scrap.select().where(((Scrap.kc_status.is_null(True)) | (Scrap.kc_status != 'updated')) & (Scrap.thumb_file_unique_id != '')).limit(BATCH_LIMIT):
        if not scrap.content:
            scrap.kc_status = 'updated'
            scrap.save()
            continue

        content = clean_bj_text(scrap.content or '')
        content = clean_text(content)
        content_seg = segment_text(content)

        tag_seg = ''
        if scrap.tag:
            tag_cn_list = parse_bj_tag_for_file(scrap.tag)
            tag_seg = ' '.join(f'#{tag}' for tag in tag_cn_list)
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {scrap.id}: {content_seg}")

        record_data = {
            'source_id': scrap.id,
            'file_type': 's',
            'content': content,
            'content_seg': content_seg,
            'tag': tag_seg,
            'file_size': scrap.estimated_file_size,
            'duration': scrap.duration,
            'thumb_file_unique_id': scrap.thumb_file_unique_id,
            'thumb_hash': scrap.thumb_hash
        }

        kw, created = SoraContent.get_or_create(source_id=scrap.id, defaults=record_data)

        if not created:
            for key, value in record_data.items():
                setattr(kw, key, value)
            kw.save()

        scrap.kc_id = kw.id
        scrap.kc_status = 'updated'
        scrap.save()

        media_data = [{
            'source_bot_name': scrap.thumb_bot,
            'file_id': None,
            'thumb_file_id': scrap.thumb_file_id
        }]

        for media in media_data:
            existing = SoraMedia.select().where(
                (SoraMedia.content_id == scrap.kc_id) &
                (SoraMedia.source_bot_name == media["source_bot_name"])
            ).first()

            if existing:
                existing.file_id = media["file_id"]
                existing.thumb_file_id = media["thumb_file_id"]
                existing.save()
                print(f"  ğŸ”„ æ›´æ–° MySQL sora_media [{media['source_bot_name']}]")
            else:
                SoraMedia.create(content_id=scrap.kc_id, **media)
                print(f"  âœ… æ–°å¢ MySQL sora_media [{media['source_bot_name']}]")

        if SYNC_TO_POSTGRES and kw.id:
            sync_to_postgres(kw)
            sync_media_to_postgres(scrap.kc_id, media_data)
            print("ğŸš€ åŒæ­¥åˆ° PostgreSQL å®Œæˆ")

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()

def process_sora_update():
    import time
    ensure_connection()  # âœ… æ¨èå†™æ³•
    # DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    sora_content_rows = SoraContent.select().where(SoraContent.stage=="pending").limit(BATCH_LIMIT)
    print(f"ğŸ“¦ æ­£åœ¨å¤„ç† {len(sora_content_rows)} ç¬” sora æ•°æ®...\n")

    for row in sora_content_rows:
        source_id = row.source_id
        print(f"ğŸ” å¤„ç† source_id: {source_id}")

        content = {
            'source_id': source_id,
            'content': row.content or '',
            'owner_user_id': row.user_id,
            'source_channel_message_id': row.source_channel_message_id,
            'thumb_file_unique_id': row.thumb_file_unique_id,
            'thumb_hash': row.thumb_hash,
            'file_size': row.file_size,
            'duration': row.duration,
            'tag': row.tag,
            'file_type': row.file_type[0] if row.file_type else None,
            'plan_update_timestamp': row.plan_update_timestamp,
            'stage': row.stage
        }

        # æ’å…¥æˆ–æ›´æ–° SoraContent
        sora_content, created = SoraContent.get_or_create(source_id=source_id, defaults=content)
        if created:
            print("âœ… æ–°å¢ MySQL sora_content")
        else:
            for k, v in content.items():
                setattr(sora_content, k, v)
            sora_content.save()
            print("ğŸ”„ æ›´æ–° MySQL sora_content")

        # å»ºç«‹ SoraMediaï¼ˆä¸¤ä¸ªæœºå™¨äººæ¥æºï¼‰
        media_data = [
            {
                'source_bot_name': row.source_bot_name,
                'file_id': row.file_id,
                'thumb_file_id': row.thumb_file_id
            },
            {
                'source_bot_name': row.shell_bot_name,
                'file_id': row.shell_file_id,
                'thumb_file_id': row.shell_thumb_file_id
            }
        ]

        for media in media_data:
            existing = SoraMedia.select().where(
                (SoraMedia.content_id == sora_content.id) &
                (SoraMedia.source_bot_name == media["source_bot_name"])
            ).first()

            if existing:
                existing.file_id = media["file_id"]
                existing.thumb_file_id = media["thumb_file_id"]
                existing.save()
                print(f"  ğŸ”„ æ›´æ–° MySQL sora_media [{media['source_bot_name']}]")
            else:
                SoraMedia.create(content_id=sora_content.id, **media)
                print(f"  âœ… æ–°å¢ MySQL sora_media [{media['source_bot_name']}]")


        # æ›´æ–°åŸå§‹è¡¨çŠ¶æ€
        row.update_content = int(time.time())
        row.save()

        # åŒæ­¥åˆ° PostgreSQL
        if SYNC_TO_POSTGRES:
            sync_to_postgres(sora_content)
            sync_media_to_postgres(sora_content.id, media_data)
            print("ğŸš€ åŒæ­¥åˆ° PostgreSQL å®Œæˆ")

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()

def sync_pending_sora_to_postgres():
    if not SYNC_TO_POSTGRES:
        print("ğŸ”’ SYNC_TO_POSTGRES ä¸º Falseï¼Œè·³è¿‡ PostgreSQL åŒæ­¥",flush=True)
        return

    print("ğŸš€ å¼€å§‹åŒæ­¥ stage = 'pending' çš„ sora_content åˆ° PostgreSQL...",flush=True)
    from playhouse.shortcuts import model_to_dict

    # DB_MYSQL.connect()
    ensure_connection()  # âœ… æ¨èå†™æ³•
    DB_PG.connect()

    rows = SoraContent.select().where(SoraContent.stage == "pending").limit(BATCH_LIMIT)

    for row in rows:
        # print(f"ğŸ”„ åŒæ­¥ä¸­ï¼šsource_id = {row.source_id}")

        model_data = model_to_dict(row, recurse=False)
        # å»é™¤ä¸å¿…è¦å­—æ®µ
        for ignored in ('content_seg_tsv', 'created_at', 'updated_at'):
            model_data.pop(ignored, None)
        model_data["id"] = row.id  # å¼ºåˆ¶ä½¿ç”¨ç›¸åŒä¸»é”®

        try:
            existing = SoraContentPg.get(SoraContentPg.id == row.id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()
            # print(f"âœ… å·²æ›´æ–° PostgreSQL sora_content.id = {row.id}")
        except SoraContentPg.DoesNotExist:
            SoraContentPg.create(**model_data)
            # print(f"âœ… å·²æ–°å¢ PostgreSQL sora_content.id = {row.id}")

        # âœ… å›å†™ MySQLï¼šstage = "updated"
        row.stage = "updated"
        row.save()
        print(f"ğŸ“ å·²æ›´æ–°ï¼šsource_id{row.source_id} =>MySQL sora_content.stage = 'updated'",flush=True)


    DB_MYSQL.close()
    DB_PG.close()


if __name__ == "__main__":
    process_documents()
    process_videos()
    process_scrap()
    sync_pending_sora_to_postgres()  # âœ… æ–°å¢çš„åŒæ­¥é€»è¾‘
