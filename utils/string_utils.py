import re
import json
from typing import List
import os

class LZString:
    # --- å°å·¥å…·ï¼šå®‰å…¨è½¬å­—ç¬¦ä¸² ---
    @staticmethod
    def _to_text(x) -> str:
        if x is None:
            return ""
        if isinstance(x, type) or callable(x):  # é¿å…æŠŠç±»/å‡½æ•°å½“æˆå­—ç¬¦ä¸²
            return ""
        if not isinstance(x, str):
            try:
                return str(x)
            except Exception:
                return ""
        return x


import json
import re
from typing import Iterable

class LZString:
    _ZERO_WIDTH = re.compile(r'[\u200B-\u200F\uFEFF]')
    _URL = re.compile(r'https?://[^\s)>\]]+')
    _TME = re.compile(r'https?://t\.me/[^\s)>\]]+', re.IGNORECASE)
    _FLAT_JSON = re.compile(r'\{[^{}]{1,2000}\}')  # é˜²æ­¢æé•¿æ–‡æœ¬å¡é¡¿
    _BLANK_LINE = re.compile(r'^[ \t]*$', re.MULTILINE)

    _AD_CUT_MARKS: tuple[str, ...] = (
        "- Advertisement - No Guarantee",
        "- å¹¿å‘Š - æ— æ‹…ä¿",
    )

    _NOISE_PHRASES: tuple[str, ...] = (
        "æ±‚æ‰“èµ", "æ±‚èµ", "å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æˆ–åˆ†äº«æ–‡ä»¶",
        "âœ…å…±æ‰¾åˆ° 1 ä¸ªåª’ä½“",
        "ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº @datapanbot å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
        "â‘ ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº  å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
        "å•æœºå¤åˆ¶ï¼š", "æ–‡ä»¶è§£ç å™¨:", "æ‚¨çš„æ–‡ä»¶ç å·²ç”Ÿæˆï¼Œç‚¹å‡»å¤åˆ¶ï¼š",
        "æ‰¹é‡å‘é€çš„åª’ä½“ä»£ç å¦‚ä¸‹:", "æ­¤æ¡åª’ä½“åˆ†äº«link:",
        "å¥³ä¾…æœç´¢ï¼š@ seefilebot", "è§£ç ï¼š@ MediaBK2bot",
        "å¦‚æœæ‚¨åªæ˜¯æƒ³å¤‡ä»½ï¼Œå‘é€ /settings å¯ä»¥è®¾ç½®å…³é—­æ­¤æ¡å›å¤æ¶ˆæ¯",
        "åª’ä½“åŒ…å·²åˆ›å»ºï¼", "æ­¤åª’ä½“ä»£ç ä¸º:", "æ–‡ä»¶åç§°:", "åˆ†äº«é“¾æ¥:", "|_SendToBeach_|",
        "Forbidden: bot was kicked from the supergroup chat",
        "Bad Request: chat_id is empty",
    )

    _TEMPLATE_PATTERNS: tuple[re.Pattern, ...] = tuple(
        re.compile(p, re.IGNORECASE) for p in (
            r'LINK\s*\n[^\n]+#C\d+\s*\nOriginal:[^\n]*\n?',
            r'LINK\s*\n[^\n]+#C\d+\s*\nForwarded from:[^\n]*\n?',
            r'LINK\s*\n[^\n]*#C\d+\s*',
            r'Original caption:[^\n]*\n?',
        )
    )

    @staticmethod
    def _to_text(s) -> str:
        return "" if s is None else str(s)

    @staticmethod
    def _cut_at_any(hay: str, marks: Iterable[str]) -> str:
        cut = len(hay)
        for m in marks:
            p = hay.find(m)
            if p != -1:
                cut = min(cut, p)
        return hay[:cut]

    @staticmethod
    def clean_text(original_string: str) -> str:
        s = LZString._to_text(original_string)

        # 0) å½’ä¸€åŒ–æ¢è¡Œ & å»é›¶å®½å­—ç¬¦
        s = s.replace('\r\n', '\n').replace('\r', '\n')
        s = LZString._ZERO_WIDTH.sub('', s)

        # 1) æˆªæ–­å¹¿å‘Šå—
        s = LZString._cut_at_any(s, LZString._AD_CUT_MARKS)

        # 2) æ‰¹é‡ç§»é™¤å™ªå£°çŸ­è¯­
        for t in LZString._NOISE_PHRASES:
            if t in s:
                s = s.replace(t, "")

        # 3) å»æ‰åˆ†äº«åˆ°æœŸæç¤º
        s = re.sub(r"åˆ†äº«è‡³\d{4}-\d{2}-\d{2} \d{2}:\d{2} åˆ°æœŸåæ‚¨ä»å¯é‡æ–°åˆ†äº«", "", s)

        # 4) å°è¯•å¤šæ®µæ‰å¹³ JSON æŠ½å– content/text
        def _json_repl(m):
            block = m.group(0)
            try:
                data = json.loads(block)
            except json.JSONDecodeError:
                return ""
            text_parts = []
            if isinstance(data, dict):
                c = data.get('content')
                t = data.get('text')
                if isinstance(c, str) and c.strip():
                    text_parts.append(c)
                if isinstance(t, str) and t.strip():
                    # é¿å… content å’Œ text é‡å¤
                    if not text_parts or t.strip() != text_parts[-1].strip():
                        text_parts.append(t)
            return ("\n" + "\n".join(text_parts)) if text_parts else ""
        s = LZString._FLAT_JSON.sub(_json_repl, s)

        # 5) é“¾æ¥ä¸æ¨¡æ¿ç§»é™¤
        s = LZString._TME.sub('', s)      # å…ˆæ¸… t.me
        s = LZString._URL.sub('', s)      # å…¶ä»–é“¾æ¥
        for pat in LZString._TEMPLATE_PATTERNS:
            s = pat.sub('', s)

        # 6) æ¸…ç©ºç™½è¡Œã€å»é‡ã€ä¿åº
        s = LZString._BLANK_LINE.sub('', s)
        lines = [ln.strip() for ln in s.split('\n') if ln.strip()]
        uniq = list(dict.fromkeys(lines))
        result = "\n".join(uniq)

        # 7) ç‰¹å®šç¬¦å·å‰æ’å…¥æ¢è¡Œï¼ˆé¿å… \rï¼‰
        for symbol in ('ğŸ”‘', 'ğŸ’'):
            result = result.replace(symbol, '\n' + symbol)

        # 8) å‹å°¾éƒ¨å¤šä½™ç©ºç™½å¹¶æˆªæ–­
        result = result.strip()
        return result[:1500] if len(result) > 1500 else result


    @staticmethod
    def clean_text2(original_string: str) -> str:
        s = LZString._to_text(original_string)

        # 0) ç»Ÿä¸€æ¢è¡Œ & å»æ‰é›¶å®½å­—ç¬¦
        s = s.replace('\r\n', '\n').replace('\r', '\n')
        s = re.sub(r'[\u200B-\u200F\uFEFF]', '', s)


        # 1) æˆªæ–­å¹¿å‘Šå—
        for target in ["- Advertisement - No Guarantee", "- å¹¿å‘Š - æ— æ‹…ä¿"]:
            pos = s.find(target)
            if pos != -1:
                s = s[:pos]

        # 2) æ‰¹é‡æ›¿æ¢å™ªå£°çŸ­è¯­
        replace_texts = [
            "æ±‚æ‰“èµ", "æ±‚èµ", "å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æˆ–åˆ†äº«æ–‡ä»¶",
            "âœ…å…±æ‰¾åˆ° 1 ä¸ªåª’ä½“",
            "ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº @datapanbot å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
            "â‘ ç§èŠæ¨¡å¼ï¼šå°†å«æœ‰File IDçš„æ–‡æœ¬ç›´æ¥å‘é€ç»™æœºå™¨äºº  å³å¯è¿›è¡Œæ–‡ä»¶è§£æ",
            "å•æœºå¤åˆ¶ï¼š", "æ–‡ä»¶è§£ç å™¨:", "æ‚¨çš„æ–‡ä»¶ç å·²ç”Ÿæˆï¼Œç‚¹å‡»å¤åˆ¶ï¼š",
            "æ‰¹é‡å‘é€çš„åª’ä½“ä»£ç å¦‚ä¸‹:", "æ­¤æ¡åª’ä½“åˆ†äº«link:",
            "å¥³ä¾…æœç´¢ï¼š@ seefilebot", "è§£ç ï¼š@ MediaBK2bot",
            "å¦‚æœæ‚¨åªæ˜¯æƒ³å¤‡ä»½ï¼Œå‘é€ /settings å¯ä»¥è®¾ç½®å…³é—­æ­¤æ¡å›å¤æ¶ˆæ¯",
            "åª’ä½“åŒ…å·²åˆ›å»ºï¼", "æ­¤åª’ä½“ä»£ç ä¸º:", "æ–‡ä»¶åç§°:", "åˆ†äº«é“¾æ¥:", "|_SendToBeach_|",
            "Forbidden: bot was kicked from the supergroup chat",
            "Bad Request: chat_id is empty",
        ]
        for t in replace_texts:
            s = s.replace(t, "")

        # 3) å»æ‰åˆ†äº«åˆ°æœŸæç¤º
        s = re.sub(r"åˆ†äº«è‡³\d{4}-\d{2}-\d{2} \d{2}:\d{2} åˆ°æœŸåæ‚¨ä»å¯é‡æ–°åˆ†äº«", "", s)

        # 4) æå–å†…åµŒ JSON é‡Œçš„ contentï¼Œå†ç§»é™¤åŸ JSON å—
        json_pattern = re.compile(r'\{[^{}]*?"text"\s*:\s*"[^"]+"[^{}]*?\}')
        def _extract_and_strip_json(m):
            block = m.group(0)
            try:
                data = json.loads(block)
                extra = ""
                if 'content' in data and isinstance(data['content'], str):
                    extra = "\n" + data['content']
                return extra  # ç”¨ extra æ›¿æ¢æ•´ä¸ª JSON å—
            except json.JSONDecodeError:
                return ""     # è§£æå¤±è´¥å°±å½“ä½œå™ªå£°ç§»é™¤
        s = json_pattern.sub(_extract_and_strip_json, s)

        # 5) ç§»é™¤é“¾æ¥/æ¨¡æ¿æ®µ
        s = re.sub(r'https://t\.me/[^\s]+', '', s)
        for pat in [
            r'LINK\s*\n[^\n]+#C\d+\s*\nOriginal:[^\n]*\n?',
            r'LINK\s*\n[^\n]+#C\d+\s*\nForwarded from:[^\n]*\n?',
            r'LINK\s*\n[^\n]*#C\d+\s*',
            r'Original caption:[^\n]*\n?',
        ]:
            s = re.sub(pat, '', s)

        # 6) å»æ‰çº¯ç©ºç™½è¡Œï¼Œå¹¶åšå»é‡ï¼ˆä¿ç•™å…ˆå‡ºç°çš„è¡Œï¼‰
        s = re.sub(r'^\s*$', '', s, flags=re.MULTILINE)
        lines = [ln for ln in s.split('\n') if ln.strip() != ""]
        unique_lines = list(dict.fromkeys(lines))
        result = "\n".join(unique_lines)

        # 7) ç‰¹å®šç¬¦å·å‰æ’å…¥æ¢è¡Œ
        for symbol in ['ğŸ”‘', 'ğŸ’']:
            result = result.replace(symbol, '\r\n' + symbol)

        return result[:1500] if len(result) > 1500 else result



    @staticmethod
    def extract_meaningful_name(filename: str) -> str | None:
        """
        ä»æ–‡ä»¶åä¸­æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†ã€‚
        è‹¥æ— æ„ä¹‰åˆ™è¿”å› Noneã€‚
        """
        # å»é™¤å‰¯æ¡£å
        name, _ = os.path.splitext(filename)

        # å»é™¤ä¸­æ‹¬å·ã€åœ†æ‹¬å·ã€ä¸‹åˆ’çº¿ã€æ¨ªçº¿ç­‰ç¬¦å·
        s = re.sub(r"[\[\]ã€ã€‘ï¼ˆï¼‰(){}<>_+\-.,ï¼Œã€‚:;!@#%^&*~]", " ", name)

        # å»é™¤å¤šä½™ç©ºæ ¼
        s = re.sub(r"\s+", " ", s).strip()

        # è‹¥æ˜¯çº¯æ•°å­—æˆ–çº¯ç¬¦å·ï¼Œåˆ™è§†ä¸ºæ— æ„ä¹‰
        if re.fullmatch(r"[\d\s]+", s):
            return None

        # è‹¥åŒ…å«å¤§é‡æ— æ„ä¹‰çš„éšæœºå­—æ¯ï¼ˆå¦‚ aJkRzTqï¼‰
        if re.fullmatch(r"[A-Za-z]{6,}", s):
            return None

        # è‹¥ä¸­æ–‡æˆ–è‹±æ–‡æ¯”ä¾‹è¿‡ä½ï¼Œä¹Ÿè§†ä¸ºæ— æ„ä¹‰
        zh_count = len(re.findall(r"[\u4e00-\u9fff]", s))
        en_count = len(re.findall(r"[A-Za-z]", s))
        num_count = len(re.findall(r"\d", s))

        total = zh_count + en_count + num_count
        if total == 0:
            return None

        # è‹¥ä¸»è¦æ˜¯æ•°å­—æˆ–ç¬¦å·
        if num_count / (total + 1e-5) > 0.6:
            return None

        # è‹¥åªæœ‰å°‘é‡æœ‰æ•ˆå­—ç¬¦ï¼ˆå¤ªçŸ­ï¼‰
        if len(s) < 3:
            return None

        # è‹¥åŒ¹é…â€œ1080pâ€ã€â€œ4kâ€ç­‰çº¯è§†é¢‘ä¿¡æ¯ï¼Œä¹Ÿè§†ä¸ºæ— æ„ä¹‰
        if re.search(r"(1080p|720p|4k|8k|h264|x264|hevc|mp4|mkv|mov|avi|webm)", s, re.I):
            return None

        # è‹¥åŒ…å«çœ‹ä¼¼æœ‰æ„ä¹‰çš„ä¸­è‹±æ–‡å•è¯ï¼ˆä¾‹å¦‚â€œæ—…è¡Œ æ—¥è®°â€ã€â€œschool projectâ€ï¼‰
        if zh_count > 0 or re.search(r"[A-Za-z]{3,}", s):
            return s

        return None


    @staticmethod
    def dedupe_cn_sentences(text: str, min_chars: int = 6, return_removed: bool = False, strict: bool = False):
        """
        å»é™¤ä¸­æ–‡æ–‡æœ¬ä¸­çš„é‡å¤å¥å­/ç‰‡æ®µã€‚
        - æ–­å¥ï¼šæŒ‰ ã€‚ï¼ï¼Ÿ!? ä¸æ¢è¡Œ
        - strict=Falseï¼šè‹¥â€œå½“å‰å¥(å»ç©ºç™½/æ ‡ç‚¹å)â€åœ¨å‰æ–‡å‡ºç°è¿‡ï¼ˆä½œä¸ºå­ä¸²ï¼‰ï¼Œåˆ™åˆ 
        - strict=True ï¼šä»…åˆ é™¤â€œå®Œå…¨ç›¸åŒâ€çš„é‡å¤å¥ï¼ˆå¿½ç•¥ç©ºç™½ä¸æ ‡ç‚¹åçš„ç›¸ç­‰ï¼‰
        """
        t = LZString._to_text(text)

        # â€”â€”æ–­å¥ & å½’ä¸€åŒ–â€”â€”
        def _split_cn_sentences(s: str) -> List[str]:
            terms = set("ã€‚ï¼ï¼Ÿ!?")
            sents, buf = [], []
            for ch in s:
                buf.append(ch)
                if ch in terms or ch == "\n":
                    sents.append("".join(buf))
                    buf = []
            if buf:
                sents.append("".join(buf))
            return sents

        _rm_ws = re.compile(r"\s+")
        _rm_punct = re.compile(r"[ã€‚ï¼ï¼Ÿ!?â€¦â‹¯ï¼Œ,ã€ï¼›;ï¼š:\n\r]+")
        def _strip_all(s: str) -> str:
            return _rm_punct.sub("", _rm_ws.sub("", s))

        sents = _split_cn_sentences(t)

        keep_mask = []
        if strict:
            seen = set()
            for s in sents:
                key = _strip_all(s)
                if not key or len(key) < min_chars:
                    keep_mask.append(True); continue
                if key in seen:
                    keep_mask.append(False)
                else:
                    seen.add(key); keep_mask.append(True)
        else:
            # ä¸ºäº†é¿å… O(n^2) ä¸²æ¥ï¼Œå¯ç´¯è®¡å‰ç¼€ï¼ˆç®€å•å®ç°å…ˆä¿ç•™ä½ çš„å†™æ³•ï¼‰
            for i, s in enumerate(sents):
                content = _strip_all(s)
                if not content or len(content) < min_chars:
                    keep_mask.append(True); continue
                prefix_clean = _strip_all("".join(sents[:i]))
                keep_mask.append(content not in prefix_clean)

        # èšåˆè¿ç»­é‡å¤å¥
        removed_groups, cur = [], []
        for s, keep in zip(sents, keep_mask):
            if not keep:
                cur.append(s.strip())
            else:
                if cur:
                    removed_groups.append("".join(cur).strip()); cur = []
        if cur:
            removed_groups.append("".join(cur).strip())

        cleaned = "".join(s for s, keep in zip(sents, keep_mask) if keep)

        if return_removed:
            seen_keys, uniq_groups = set(), []
            for g in removed_groups:
                k = _strip_all(g)
                if k and k not in seen_keys:
                    uniq_groups.append(g); seen_keys.add(k)
            return cleaned, uniq_groups
        return cleaned

    def shorten_text(text: str, max_length: int = 30) -> str:
        if not text:
            return ""
        return text[:max_length] + "..." if len(text) > max_length else text